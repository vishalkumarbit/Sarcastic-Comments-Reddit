{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\80316\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [24/Jun/2022 11:50:50] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            comment       author        subreddit  score  ups  \\\n",
      "0  you should file a dmca complaint  BrokenStool  GlobalOffensive      1    1   \n",
      "\n",
      "   downs                                     parent_comment  \n",
      "0      0  This skin looks exactly like the SA Highwayman...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Jun/2022 11:51:36] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/deploy-machine-learning-model-using-flask/#:~:text=%23%20prediction%20function,prediction%20%3D%20prediction)\n",
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing libraries\n",
    "import datetime\n",
    "from flask import Flask, jsonify, request\n",
    "from flask import Flask, render_template\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def decontracted(phrase):\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "\n",
    "        # https://gist.github.com/sebleier/554280\n",
    "\n",
    "        # Remove Emoji\n",
    "    def deEmojify(text):\n",
    "        regrex_pattern = re.compile(pattern = \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "        return regrex_pattern.sub(r'',text)\n",
    "\n",
    "    def preprocess_text(text_data):\n",
    "        preprocessed_text = []\n",
    "        # tqdm is for printing the status bar\n",
    "        for sentance in tqdm(text_data):\n",
    "            sent = decontracted(sentance)\n",
    "            sent=deEmojify(sent)\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            # https://gist.github.com/sebleier/554280\n",
    "            sent = ' '.join(e for e in sent.split() if e not in stop_words)\n",
    "            preprocessed_text.append(sent.lower().strip())\n",
    "        return preprocessed_text\n",
    "    \n",
    "    data['comment'] = preprocess_text(data['comment'].values)\n",
    "    data['parent_comment'] = preprocess_text(data['parent_comment'].values)\n",
    "        \n",
    "    data['subreddit'] = data['subreddit'].str.replace(' ','')\n",
    "    data['subreddit'] = data['subreddit'].str.replace('-','_')\n",
    "    data['subreddit'] = data['subreddit'].str.lower()\n",
    "    \n",
    "    data['author'] = data['author'].str.replace(' ','')\n",
    "    data['author'] = data['author'].str.replace('-','_')\n",
    "    data['author'] = data['author'].str.lower()\n",
    "    \n",
    "    neg_comment = []\n",
    "    neu_comment = []\n",
    "    pos_comment = []\n",
    "    comp_comment = []\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for sentence in tqdm(data['comment']):\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        neg_comment.append(ss['neg'])\n",
    "        neu_comment.append(ss['neu'])\n",
    "        pos_comment.append(ss['pos'])\n",
    "        comp_comment.append(ss['compound'])\n",
    "\n",
    "    data['Neg_comment'] = neg_comment\n",
    "    data['Neu_comment'] = neu_comment\n",
    "    data['Pos_comment'] = pos_comment\n",
    "    data['Compound_comment'] = comp_comment\n",
    "    \n",
    "    neg_pcomment = []\n",
    "    neu_pcomment = []\n",
    "    pos_pcomment = []\n",
    "    comp_pcomment = []\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for sentence in tqdm(data['parent_comment']):\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        neg_pcomment.append(ss['neg'])\n",
    "        neu_pcomment.append(ss['neu'])\n",
    "        pos_pcomment.append(ss['pos'])\n",
    "        comp_pcomment.append(ss['compound'])\n",
    "    \n",
    "    data['Neg_pcomment'] = neg_pcomment\n",
    "    data['Neu_pcomment'] = neu_pcomment\n",
    "    data['Pos_pcomment'] = pos_pcomment\n",
    "    data['Compound_pcomment'] = comp_pcomment\n",
    "    \n",
    "    data['comment_len']=data['comment'].apply(lambda x:len(x.split()))\n",
    "    data['pcomment_len']=data['parent_comment'].apply(lambda x:len(x.split()))\n",
    "    \n",
    "    cat_author, cat_subreddit = pickle.load(open(\"cat.pkl\", 'rb'))\n",
    "    data['author'] = cat_author.transform(data['author'])\n",
    "    data['subreddit'] = cat_subreddit.transform(data['subreddit'])\n",
    "    \n",
    "    scaler_score, scaler_ups, scaler_negc, scaler_neuc, scaler_posc, scaler_compc, scaler_negpc, scaler_neupc, scaler_pospc, scaler_comppc, scaler_comlen, scaler_pcomlen = pickle.load(open(\"scale.pkl\", 'rb'))\n",
    "    data['score'] = scaler_score.transform(data['score'].values.reshape(-1,1))    \n",
    "    data['ups'] = scaler_ups.transform(data['ups'].values.reshape(-1,1))\n",
    "    data['Neg_comment'] = scaler_negc.transform(data['Neg_comment'].values.reshape(-1,1))\n",
    "    data['Neu_comment'] = scaler_neuc.transform(data['Neu_comment'].values.reshape(-1,1))\n",
    "    data['Pos_comment'] = scaler_posc.transform(data['Pos_comment'].values.reshape(-1,1))\n",
    "    data['Compound_comment'] = scaler_compc.transform(data['Compound_comment'].values.reshape(-1,1))\n",
    "    data['Neg_pcomment'] = scaler_negpc.transform(data['Neg_pcomment'].values.reshape(-1,1))\n",
    "    data['Neu_pcomment'] = scaler_neupc.transform(data['Neu_pcomment'].values.reshape(-1,1))\n",
    "    data['Pos_pcomment'] = scaler_pospc.transform(data['Pos_pcomment'].values.reshape(-1,1))\n",
    "    data['Compound_pcomment'] = scaler_comppc.transform(data['Compound_pcomment'].values.reshape(-1,1))\n",
    "    data['comment_len'] = scaler_comlen.transform(data['comment_len'].values.reshape(-1,1))\n",
    "    data['pcomment_len'] = scaler_pcomlen.transform(data['pcomment_len'].values.reshape(-1,1))\n",
    "    \n",
    "    text_features = data[['comment', 'parent_comment']]\n",
    "    data.drop(['comment', 'parent_comment'], axis=1, inplace=True)\n",
    "    \n",
    "    num_features = data.values\n",
    "    \n",
    "    token_comment, token_pcomment =  pickle.load(open('tokens.pkl','rb'))\n",
    "    sequence_com = token_comment.texts_to_sequences(text_features['comment'])\n",
    "    pad_comment = pad_sequences(sequence_com, maxlen=50, padding='post')\n",
    "    sequence_pcom = token_pcomment.texts_to_sequences(text_features['parent_comment'])\n",
    "    pad_pcomment = pad_sequences(sequence_pcom, maxlen=500, padding='post')    \n",
    "    \n",
    "    return pad_comment, pad_pcomment, num_features\n",
    "\n",
    "model = keras.models.load_model('final_model.h5')\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():        \n",
    "    df = pd.DataFrame()\n",
    "    df['comment'] = list([str(request.form['comment'])])\n",
    "    df['author'] = list([str(request.form['author'])])\n",
    "    df['subreddit'] = list([str(request.form['subreddit'])])\n",
    "    df['score'] = list([int(request.form['score'])]) \n",
    "    df['ups'] = list([int(request.form['ups'])])\n",
    "    df['downs'] = list([int(request.form['downs'])])\n",
    "    df['parent_comment'] = list([str(request.form['parent_comment'])]) \n",
    "    \n",
    "    print(df)\n",
    "        \n",
    "    final_data = preprocessing(df)    \n",
    "    \n",
    "    y_pred = model.predict(final_data)\n",
    "    \n",
    "    if y_pred >= 0.5:\n",
    "        prediction = 'Sarcastic'\n",
    "    else:\n",
    "        prediction = 'Non-Sarcastic'\n",
    "    return render_template(\"index.html\", prediction_text = 'Comment on the post is {}'.format(prediction))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
